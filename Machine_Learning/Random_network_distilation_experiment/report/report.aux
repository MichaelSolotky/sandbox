\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pitfalls}
\citation{pitfalls}
\citation{pitfalls}
\citation{ensembles}
\citation{pitfalls}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Paper review}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Ensembles of neural networks}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Metrics for in-domain uncertainty quantification}{1}{subsection.2.2}\protected@file@percent }
\citation{end2}
\citation{transformer}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Test-time-augmentation}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Overall evaluation}{2}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Popular methods for OoD detection}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian ensembles from different points of view}{2}{subsection.3.1}\protected@file@percent }
\citation{pitfalls}
\citation{structured_prediction}
\citation{structured_prediction}
\citation{structured_prediction}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A toy example that shows how decision boundaries learned by different neural nets of an ensemble may differ in the areas which are far from the training data distribution. Red and blue points represent objects of negative and positive classes respectively and green points represent OoD samples}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:decision_boundaries}{{1}{3}{A toy example that shows how decision boundaries learned by different neural nets of an ensemble may differ in the areas which are far from the training data distribution. Red and blue points represent objects of negative and positive classes respectively and green points represent OoD samples}{figure.1}{}}
\citation{autoencoders}
\citation{vae}
\citation{vae}
\citation{do_dont_know}
\citation{do_dont_know}
\citation{norm_flows_fail}
\citation{transformer}
\citation{do_dont_know}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Label-free models}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Reconstruction based models}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Generative models}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}OoD detection in NMT}{4}{section.4}\protected@file@percent }
\citation{random_network}
\citation{do_dont_know,norm_flows_fail}
\@writefile{toc}{\contentsline {section}{\numberline {5}Random Network Distilation}{5}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The training history of RND on the Fashion MNIST $\rightarrow $ MNIST direction. The training was set to 500 epochs. Although the process didn't converge, the eventual performance is predictable from difference of errors on the training and testing sets and is close to ideal}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:learning_on_fashion}{{2}{5}{The training history of RND on the Fashion MNIST $\rightarrow $ MNIST direction. The training was set to 500 epochs. Although the process didn't converge, the eventual performance is predictable from difference of errors on the training and testing sets and is close to ideal}{figure.2}{}}
\citation{end2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The training history of RND on the MNIST $\rightarrow $ Fashion MNIST direction}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:learning_on_original}{{3}{6}{The training history of RND on the MNIST $\rightarrow $ Fashion MNIST direction}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of detection quality of autoencoder and RND on MNIST and Fashion MNIST. It seems interesting these models show exceptional quality on different dataset pairs. We can't say one model is universally better.}}{6}{table.1}\protected@file@percent }
\newlabel{table}{{1}{6}{Comparison of detection quality of autoencoder and RND on MNIST and Fashion MNIST. It seems interesting these models show exceptional quality on different dataset pairs. We can't say one model is universally better}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Future Work}{6}{section.6}\protected@file@percent }
\citation{random_network}
\bibstyle{unsrt}
\bibdata{report}
\bibcite{pitfalls}{{1}{}{{}}{{}}}
\bibcite{ensembles}{{2}{}{{}}{{}}}
\bibcite{end2}{{3}{}{{}}{{}}}
\bibcite{transformer}{{4}{}{{}}{{}}}
\bibcite{structured_prediction}{{5}{}{{}}{{}}}
\bibcite{autoencoders}{{6}{}{{}}{{}}}
\bibcite{vae}{{7}{}{{}}{{}}}
\bibcite{do_dont_know}{{8}{}{{}}{{}}}
\bibcite{norm_flows_fail}{{9}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{7}{section.7}\protected@file@percent }
\bibcite{random_network}{{10}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
