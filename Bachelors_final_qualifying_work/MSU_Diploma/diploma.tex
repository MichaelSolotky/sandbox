\documentclass[12pt,fleqn,unicode]{article}

\usepackage[all]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{cmap}       % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage[cp1251]{inputenc}
\usepackage[footnotesize]{caption2}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage[russian]{babel}
\usepackage{setspace}
\usepackage{theorem}
\usepackage[T2A]{fontenc}
\usepackage{xcolor}

% Параметры страницы
\textheight=240mm
\textwidth=160mm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-14mm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.3} %для печати с большим интервалом

% Полезные команды для математики
\newcommand{\const}{\mathrm{const}}
\newcommand{\tsum}{\mathop{\textstyle\sum}\limits}
\newcommand{\tprod}{\mathop{\textstyle\prod}\limits}
\newcommand{\argmin}{\mathop{\rm arg\,min}\limits}
\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}
\newcommand{\sign}{\mathop{\rm sign}\limits}
\newcommand{\norm}{\mathop{\rm norm}\limits}
\newcommand{\SoftMax}{\mathop{\rm SoftMax}\limits}
\newcommand{\KL}{\mathop{\rm KL}\nolimits}
\newcommand{\Dir}{\mathop{\rm Dir}\nolimits}
\newcommand{\rank}{\mathop{\rm rank}\nolimits}
\newcommand{\diag}{\mathop{\rm diag}\nolimits}
\newcommand{\scal}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\BigARTM}{\texttt{BigARTM}}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\phi}{\varphi}
\newcommand{\T}{\textsf{\upshape т}}
\newcommand{\eps}{\varepsilon}
\newcommand{\emset}{\varnothing}
\newcommand{\cond}{\mspace{2mu}{|}\mspace{2mu}}
\newcommand{\Loss}{\mathscr{L}}
\newcommand{\Expect}{\mathsf{E}}
\newcommand{\Disp}{\mathsf{D}}
\newcommand{\Var}{\mathsf{D}}
\newcommand\RR{\mathbb{R}}
\newcommand\DD{\mathbb{D}}
\newcommand\cL{\mathscr{L}}
\newcommand\cF{\mathscr{F}}
\newcommand\cG{\mathscr{G}}
\newcommand\cJ{\mathcal{J}}
\newcommand\cN{\mathcal{N}}
\newcommand\cB{\mathscr{B}}
\newcommand\cK{\mathscr{K}}
\newcommand\cP{\mathscr{P}}
\newcommand\cC{\mathscr{C}}
\newcommand\cS{\mathscr{S}}
\renewcommand{\em}{\it}
\def\afterlabel#1{\renewcommand\labelenumi{\theenumi #1}}

% Оформление окружений типа теорем
\theoremstyle{plain}
% Шаманские притопывания, чтобы ставить точку после номера теоремы
\makeatletter
\gdef\th@plain{\normalfont
    \def\@begintheorem##1##2{%
        \item[\hskip\labelsep\theorem@headerfont ##1\ ##2. ]}%
    \def\@opargbegintheorem##1##2##3{%
        \item[\hskip\labelsep\theorem@headerfont ##1\ ##2 (##3). ]}%
}
\makeatother
\theorembodyfont{\rmfamily\slshape}
\newtheorem{Theorem}{Теорема}
\newtheorem{Lemma}{Лемма}
\newtheorem{Corr}{Следствие}

\newenvironment{Proof}%
    %{\par\noindent{\bf Доказательство.\par\nopagebreak}}%
    {\par\noindent{\bf Доказательство.~}}%
    {\hfill$\scriptstyle\blacksquare$\par\medskip}

% Оформление алгоритмов в пакете algorithm2e
\makeatletter
\renewcommand{\algorithmcfname}{Этап}
\renewcommand{\listalgorithmcfname}{Список алгоритмов}
\renewcommand{\@algocf@procname}{Процедура}
\renewcommand{\@algocf@funcname}{Функция}
\renewcommand{\procedureautorefname}{процедура}
\renewcommand{\functionautorefname}{функция}
\newcommand{\vkKw}[1]{\textsf{#1}}
\SetKwInput{KwIn}{\vkKw{Вход}}
\SetKwInput{KwOut}{\vkKw{Выход}}
\SetKwInput{KwData}{\vkKw{Вход}}
\SetKwInput{KwResult}{\vkKw{Выход}}
\SetKw{KwRet}{\vkKw{выход}}
\SetKw{Return}{\vkKw{выход}}
\SetKwIF{If}{ElseIf}{Else}{\vkKw{если}}{\vkKw{то}}{\vkKw{иначе если}}{\vkKw{иначе}}{}
\SetKwRepeat{Repeat}{\vkKw{повторять}}{\vkKw{пока}}
\SetKwFor{For}{\vkKw{для}}{}{}
\SetKwFor{ForAll}{\vkKw{для всех}}{}{}
\SetKwFor{ForEach}{\vkKw{для всех}}{}{}
\SetKwFor{While}{\vkKw{пока}}{}{}
\setlength{\interspacetitleruled}{4pt}
\SetAlgoCaptionSeparator{.}
\SetAlgoInsideSkip{smallskip}
\SetAlCapHSkip{0ex}
\SetArgSty{relax}
\makeatother

% Оформление листингов
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}
\lstdefinestyle{python}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{blue}\slshape,
    keywordstyle=\color{codegreen},
    stringstyle=\color{codepurple},
    %basicstyle=\ttfamily\scriptsize,
    basicstyle=\ttfamily\small,
    %basicstyle=\ttfamily\footnotesize\setstretch{0.9},
    columns=flexible,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numberstyle=\tiny\color{codegray},
    numbersep=2ex,
    xleftmargin=\parindent,
    framextopmargin=2em,
    framexbottommargin=2em,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=python}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{titlepage}
\begin{center}
	Московский государственный университет имени М. В. Ломоносова

	\bigskip
	\includegraphics[width=50mm]{msu.eps}

	\bigskip
	Факультет Вычислительной Математики и Кибернетики\\
	Кафедра Математических Методов Прогнозирования\\[10mm]

	\textsf{\large\bfseries
		ВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА\\[10mm]
		\bigskip
		Вероятностные тематические модели на основе данных о со-встречаемости слов
	}\\[12mm]

    \bigskip
	\begin{flushright}
		\parbox{0.5\textwidth}{
			Выполнил:\\
			студент 4 курса 417 группы\\
			\emph{Солоткий Михаил}\\[5mm]
			Научный руководитель:\\
			д.ф-м.н., профессор\\
			\emph{Воронцов Константин Вячеславович}
		}
	\end{flushright}

	\vspace{\fill}
	Москва, 2019
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage
\begin{abstract}
    ToDo: переделай \\
	В данной работе приводится параллельный алгоритм сбора статистики со-встречаемостей токенов в больших текстовых коллекциях, таких как англоязычная Википедия, а также эксперименты по измерению когерентности тематических моделей.
\end{abstract}

\section{Введение}
    ToDo: переделай \\
	Тематическое моделирование — это
\newpage

\section{Тематическая модель PLSA}
	Пусть $D$ --- конечное множество документов (коллекция, корпус), $W$ --- конечное множество токенов данной коллекции (словарь). Обычно токенами являются слова, но могут быть и словосочетания. Предполагается, что появление произвольного токена $w$ в произвольном документе $d$ связано с некоторой темой $t$, принадлежащей конечному множеству тем $T$. Следующими модельными предположением является гипотеза <<мешка слов>>, которая утверждает, что для выявления тематики документа не важен порядок токенов в документе, а важны лишь частоты вхождения токенов. Также вводится гипотеза <<мешка документов>>, которая говорит, что для выявления тематики коллекции не важен порядок документов. Эти две гипотезы дают возможность построить вероятностную модель порождения текстовой коллекции. Пространством элементарных исходов является $\Omega = W \times D \times T$, а вся коллекция рассматривается как простая выборка троек ($w_i$, $d_i$, $t_i$) из категориального распределения $\Prob(w, d, t)$, причём темы токенов $t_i$ являются скрытыми, а наблюдаются пары $(w_i, d_i)$. Предполагается, что вероятность появления токена в документе связана с его темой и не связана с документом, в котором он встретился. Формально это записывается так: $\Prob(w | d, t) = \Prob(w | t)$. Таким образом тема есть вероятностное распределение на $W$.

	Модель PLSA (Probabilistic Latent Semantic Analysis) \cite{plsa}:
	$$\Prob(w|d) = \sum_{t \in T} \Prob(w | t) \Prob(t | d) = \sum_{t \in T} \Phi_{wt} \Theta_{td}$$

    Каждой позиции каждого токена в каждом документе присваивается своя скрытая переменная $t$ --- тема данного токена на данной позиции в данном документе. \\
    Далее будут использоваться следующие обозначения (аналогичные \cite{artm}): \\ \\
    $n_{dwt}$ --- счётчик, сколько раз токен $w$ был ассоциирован с темой $t$ в документе $d$; \\
    $n_{wt} = \sum_{d} n_{wt}$ --- счётчик, сколько раз в коллекции токен $w$ ассоциирован с темой $t$; \\
    $n_{dt} = \sum_{w} n_{dwt}$ --- количество токенов из документа $d$, ассоциированные с темой $t$; \\
    $n_{dw} = \sum_{t} n_{dwt}$ --- количество вхождений токена $w$ в документ $d$; \\
    $n_{d} = \sum_{wt} n_{dwt}$ --- длина документа $d$; \\
    $n_{w} = \sum_{dt} n_{dwt}$ --- количество вхождений токена $w$ в коллекцию; \\
    $n_{t} = \sum_{dw} n_{dwt}$ --- количество позиций токенов в коллекции, принадлежащих теме $t$. \\ \\
    Таким образом можно выразить через эти счётчики элементы матриц $\Phi, \Theta$: $$\phi_{wt} = \frac{n_{wt}}{n_{t}},
    \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
     \theta_{td} = \frac{n_{td}}{n_d}$$
    При этом мы наблюдаем частотные оценки $\hat{\Prob}(w | d) = \cfrac{n_{dw}}{\sum\limits_{w \in d} n_{dw}} = \cfrac{n_{dw}}{n_d}$.
	Задача состоит в восстановлении матриц $\Phi$ и $\Theta$ по выборке, то есть в нахождении описания токенов и текстовых документов с помощью смеси тем.
	Восстановление матриц происходит посредством максимизации неполного правдоподобия с помощью EM-алгоритма:
	$$\mathcal{L}(\Phi, \Theta) = \sum_{d, w} n_{dw} \ln \sum_{t \in T} \phi_{wt} \theta_{td} \rightarrow \max_{\Phi, \Theta}$$

	Одной из популярных в компьютерной лингвистике мер качества оценки языковых моделей является перплексия. Применительно к модели PLSA она принимает вид:
	$$\mathcal{P}(D) = \exp \bigg( -\frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{wd} \ln \Prob(w|d) \bigg)$$
	По сути формула представляет собой усреднённый по всем токенам коллекции логарифм правдоподобия, от которого затем взята обратная экспонента.

\section{Аддитивная Регуляризация Тематических Моделей}
    Исходная модель PLSA обладает неединственностью решения, так как для матриц $\Phi$, $\Theta$ можно подобрать такую стохастическую матрицу $S$ ранга $|T|$ и объявить матрицы $\Phi^{'} = \Phi S$, $\Theta^{'} = S^{-1} \Theta$~ новым решением. Это некорректно поставленная задача, которую можно пытаться подменить корректно поставленной с помощью добавления регуляризаторов к исходному функционалу \cite{artm}:
    $$\mathcal{L}(\Phi, \Theta) = \sum_{d, w} n_{dw} \ln \sum_{t \in T} \phi_{wt} \theta_{td} + \sum_{i=1}^k R_i(\Phi, \Theta) \rightarrow \max_{\Phi, \Theta}$$

    В литературе в основном решают проблему неединственности решения PLSA с помощью введения априорных распределений на матрицы $\Phi$ и $\Theta$ и используются техники приближённого байесовского вывода для обучения моделей. Модель называется LDA (Latent Dirichlet Allocation) \cite{lda}. Большинство известных тематических моделей, в том числе LDA, допускают переформулировку в терминах аддитивной регуляризации, и могут обучаться посредством максимизации неполного регуляризованного правдоподобия, 

\section{Интерпретируемость тем}
	Недостатком рассмотренных тематических моделей является то, что они никак не учитывают взаимное расположение токенов в коллекции. Согласно гипотезе дистрибутивности \cite{harris}: токены близки семантически, если совместно часто встречаются близко в тексте. Информация о том, как часто некоторые токены встречаются рядом в коллекции могла бы помочь построить более точную модель с более интерпретируемыми темами. Интерпретируемость --- субъективное понятие. Процесс оценивания темы выглядит следующим образом: эксперт в некоторой предметной области получает топ $k$ токенов некоторой темы (обычно $k = 10$) и должен без труда понять, что связывает полученные токены и дать теме адекватное название. Затем по некоторое шкале выставляет оценку: насколько тема интерпретируема.

\section{Когерентность тем}
	В \cite{newman} проводились попытки подобрать автоматически вычисляемую меру качества темы, которая бы коррелировала с оценками, выставленными экспертами (в смысле корреляции Спирмена). Лучшей мерой качества оказалась когерентность --- средняя PMI (Pointwise Mutual Information) \cite{pmi-origins} по парам среди m наиболее вероятным различным токенам темы.
	$$ C_t = \frac{2}{m(m-1)} \sum_{i=1}^{m-1} \sum_{j=i+1}^m \text{PMI} (w_i, w_j)$$
	$$ \text{PMI} (A, B) = \log \frac{\Prob (A \cap B)}{\Prob(A) \Prob(B)} $$
	Также в \cite{bullinaria, levy} было показано, что использование Positive PMI вместо PMI и Shifted Positive PMI улучшает качество в задачах семантической близости слов.
	$$ \text{PPMI} (A, B) = \max (0, \text{PMI}(A, B)) $$
	$$ \text{SPPMI}_k (A, B) = \max (0, \text{PMI}(A, B) - \ln k) $$
	Если события $A$ и $B$ независимы, то вероятность пересечения событий факторизуется в произведение вероятностей по отдельным событиям и PPMI равна 0. То есть данная мера близости присваивает значения больше 0 парам, которые встречаются вместе чаще, чем если бы их появление было независимо. Идея Shifted PMI заключается в удалении из рассмотрения пар, которые чуть-чуть чаще встречаются вместе, чем случайно. Такие пары неинтересны, и за счёт их удаления можно повысить эффективность вычисления когерентности и уменьшить затраты по памяти.

    Ясно, что понятие когерентности определено неоднозначно, так как совместную вероятность токенов может рассчитывать по-разному в зависимости от модели. Далее рассматриваются некоторые модели со-встречаемостей токенов.

\section{Модели со-встречаемостей токенов}
\subsection{PPMI, основанная на частотах пар токенов}
    Рассмотрим в документе $d$ множество позиций токенов. Скажем, что пара позиций ($i$, $j$) в одном и том же документе находится в некотором окне ширины $k$, если $ 0 < |i - j| \leq k $. Через $w_{di}$ обозначим токен, находящийся на позиции $i$ в документе $d$. Через $n_{uv}$ - счётчик со-встречаемости токенов $u$ и $v$ обозначим количество позиций ($i$, $j$) суммарно во всех документах коллекции, принадлежащих некоторому окну и таких, что $w_{di} = u$, $w_{dj} = v$. Формально можно записать следующим образом:
    $$n_{uv} = \sum_{d = 1}^{|D|} \sum_{i = 1}^{n_d} \sum_{j = 1}^{n_d}
			[0 < |i - j| \leq k] [w_{di} = u] [w_{dj} = v] $$
    Заметим, что $n_{uv} = n_{vu}$. \\
    Введём вероятность на множестве пар токенов:
    $$\Prob(u, v) \propto n_{uv}$$
    $\Prob(u, v)$ --- совместная вероятность наблюдения пары токенов $(u, v)$, она же вероятность пересечения событий --- наблюдение токена $u$ и наблюдение токена $v$.
    Обозначив через $n_{u} = \sum\limits_{v} n_{uv}$ и через $n = \sum\limits_{u} n_u$, получаем:
    $$\Prob(u, v) = \frac{n_{uv}}{n}
    \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
    \Prob (u) = \sum_{v} \Prob (u, v) = \frac{n_u}{n}$$
    $\Prob (u)$ примерно равна доле токена $u$ среди всех токенов коллекции, равенство не точное за счёт краевых эффектов на границах документов.
    Итоговая формула для PPMI:
    $$ \text{PPMI}(u, v) = \Bigg[ \ln \frac{\Prob(u, v)}{\Prob(u) \Prob(v)} \Bigg]_{+} = \max \Big( 0, \ln \Big[ \frac{n_{uv} \cdot n}{n_u \cdot n_v} \Big] \Big)$$
    Данная модель с небольшими отличиями использовалась в \cite{bullinaria, levy, levy2}.

\subsection{PPMI, основанная на частотах документов}
	Ещё одиним популярным способом задания вероятностей для PPMI является учёт количества документов, в которых фигурировала хотя бы раз данная пара токенов. В оригинальное статье \cite{mimno} не использовались окна фиксированной ширины, а учитывались попадание пары токенов в 1 документ. Этот способ можно обобщить введением окон фиксированной ширины по аналогии с предыдущей моделью, основанной на частотах пар токенов.

Введём величину $n_{uv}$, которая равна количеству документов, в которых встретились токены $u$ и $v$ хотя бы раз внутри некоторого окна ширины $k$:
$$n_{uv} = \Bigg| \Big\{ d \in D \, \big| \, \exists \, (i, j): w_{di} = u, \, w_{dj} = v, \, 0 < |i - j| \leq k \Big\} \Bigg|$$
    Элементарным исходом является документ, на множестве документов введём равномерное распределение. Тогда вероятность совместно встретить токены $u$ и $v$ внутри некоторого окна в коллекции будет равна доле документов с данным свойством, а соответственно вероятность появления токена будет также выражаться в терминах документов --- доля документов, в которых встречался данный токен хотя бы 1 раз:
	$$ \Prob (u, v) = \frac{n_{uv}}{|D|}
	\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
	\Prob (u) = \frac{n_u}{|D|} $$
	То есть количество документов нормируется на общий размер коллекции.
	Тогда PPMI можно переписать следующим образом:
	$$ \text{PPMI}(u, v) = \frac{n_{uv} \cdot |D|}{n_u \cdot n_v} $$

\section{Регуляризатор когерентности}

    В \cite{mimno} была предложена процедура оптимизации когерентности. Идеи, описанные в данной статье были заложены в основу регуляризатора когерентности, который был описан в \cite{artm}. Процедура оптимизации из статьи \cite{mimno} может быть приближённо сформулирована в виде следующего регуляризатора:
    $$R_{doc\_coher}(\Phi) = \tau \sum_{t \in T} \sum_{(u, v) \in W^2} \frac{N_{uv}}{N_v} n_{vt} \ln \phi_{vt}$$
    где $N_{uv}$ --- количество документов, в которых встретились вместе токены $u$ и $v$ (не в некотором окне, а в принципе в документе), $N_u$ --- количество документов, в которых встречается токен $u$, $n_{vt}$ --- количество раз, сколько токен $v$ был связан с темой $t$ в коллекции.
    Данный регуляризатор является частным случаем более общего:
    $$R(\Phi) = -\tau \sum_{t \in T} n_t ~\text{KL}_u \big( \hat{\Prob}(u | t) ~||~ \phi_{ut} \big) = -\tau \sum_{t \in T} n_t \sum_{u \in W} \hat{\Prob}(u | t) \ln \frac{\hat{\Prob}(u | t)}{\phi_{ut}}$$
    $$\argmax_{\Phi} R(\Phi) = \argmax_{\Phi} \Bigg[ \tau \sum_{t \in T} n_t \sum_{u \in W} \hat{\Prob}(u | t) \ln \phi_{ut} \Bigg] = $$
    $$= \argmax_{\Phi} \Bigg[ \tau \sum_{t \in T} n_t \sum_{u \in W} \hat{\Prob}(u | t) \ln \phi_{ut} \Bigg] = $$
    $$\Bigg\{ \hat{\Prob}(u | t) = \sum_{v \in W} \Prob_{doc}(u | v, t) \Prob(v | t), \,\,\, \Prob_{doc}(u | v, t) = \Prob_{doc}(u | v)  \Bigg\} = $$
    $$= \argmax_{\Phi} \Bigg[ \tau \sum_{t \in T} n_t \sum_{(u, v) \in W^2} \Prob_{doc}(u | v) \Prob(v | t) \ln \phi_{ut} \Bigg] = $$
    $$ =\argmax_{\Phi} \Bigg[ \tau \sum_{t \in T} n_t \sum_{(u, v) \in W^2} \Prob_{doc}(u | v) \, \frac{n_{vt}}{n_t} \, \ln \phi_{ut} \Bigg] =
    \argmax_{\Phi} \Bigg[ \tau \sum_{t \in T} \sum_{(u, v) \in W^2} \frac{N_{uv}}{N_v} \, n_{vt} \, \ln \phi_{ut} \Bigg] $$

    Ничего не мешает заменить $\Prob_{doc}(u | v)$ на условные вероятности другой вероятностной модели, главное --- определить в соответствующей модели счётчики $n_{uv}$, так как $$\Prob(u | v) = \cfrac{\Prob(u, v)}{\Prob(v)} = \cfrac{n_{uv}}{\sum_u n_{uv}}$$

\section{Алгоритм подсчёта со-встречаемостей}
	В данной работе предложен алгоритм обработки текстовых коллекций и подсчёта статистики со-встречаемостей пар токенов. Особенности данного алгоритма:
\begin{itemize}
	\item асинхронная обработка входной коллекции и промежуточных данных;
	\item возможность обработки коллекций потенциально неограниченных по числу документов;
	\item возможность улучшения производительности за счёт увеличения числа потоков-обработчиков на 1 вычислительном узле
\end{itemize}
	Условно алгоритм можно разбить на 3 этапа:
\begin{itemize}
	\item обработка входной коллекции;
	\item агрегация статистики, собранной по коллекции;
	\item вычисление метрик на основе статистики со-встречаемости.
\end{itemize}

\subsection{Обработка входной коллекции}
	Обработка входной коллекции происходит по батчам, т.е. в оперативную память считывается $batch\_size$ документов коллекции и считаются со-встречаемости на этом подмножестве документов, как будто они составляют всю коллекцию. Предполагается, что размер каждого документа ограничен общей константой, иначе нельзя было бы гарантировать, что каждый документ можно считать в оперативную память. Параметр $batch\_size$ указывается пользователем. Также пользователь может указать некоторый словарь релевантных для него токенов --- $vocab$, и со-встречаемости будут считаться только между токенами из данного множества. $vocab$ может представлять собой словарь коллекции $W$, очищенный от стоп-слов и низкочастотных токенов, или он может состоять из токенов некоторого узкого набора тем. Для больших коллекций рекомендуется задавать $vocab$. Дело в том, что множество всех токенов некоторого естественного языка может быть довольно большим --- около миллиона токенов или больше, и на хранение статистики по каждой паре в батче будет расходоваться много памяти. Таким образом, если задать большой $vocab$ или не задавать его вовсе, не получится использовать большие батчи, так как при этом не хватило бы памяти для хранения большого количества различных пар токенов, встретившихся в батче. Статистика со-встречаемостей по каждому батчу сохраняется в файл в отсортированном формате для того, чтобы потом можно было легко объединить файлы, соответствующие всем различным батчам в один. Также для вычисления PPMI нужно значение $n$ --- общее количество рассмотренных пар в коллекции. Это значение можно вычислить на первом этапе алгоритма. Обработка батчей происходит параллельно асинхронно в $n_{jobs}$ потоков по 1 батчу на поток. Для каждого документа подсчитываются частоты со-встречаемостей пар токенов внутри некоторого окна ширины $window\_width$, которая является параметром алгоритма. Если некоторое окно выходит за границы документа, оно усекается до соответствующих границ.

\begin{algorithm2e}[!ht]
\caption{Обработка входной коллекции.}
	\KwIn{коллекция $D$, $vocab$, $batch\_size$, $window\_width$, $n_{jobs}$\;}
	\KwOut{набор отсортированных файлов со-встречаемостей $F$, общее количество пар $n$\;}
	$n \gets 0$\;
	\While{не конец коллекции} {
		\ForAll{$job = 1, \dots, n_{jobs}$} {
			инициализировать пустой сортирующий контейнер $C$\;
			считать $batch\_size$ документов в $batch$\;
			\ForAll{$d\in batch$}{
				\ForAll{$i=1,\dots,n_d - 1$}{
					\ForAll{$j=1,\dots, window\_width: \ i + j \leq n_d$}{
						$r = i + j$\;
						$C[w_{di}, w_{dr}] \gets C[w_{di}, w_{dr}] + 1$\;
						$C[w_{dr}, w_{di}] \gets C[w_{dr}, w_{di}] + 1$\;
						$n \gets n + 2$
					}
				}
			}
			сохранить $C$ во внешнюю память;
		}
	}
\end{algorithm2e}

	В алгоритме используется сортирующий контейнер $C$. Для эффективной обработки коллекции необходимо, чтобы была возможность быстрого доступа к его элементам и их изменение. Для реализации было выбрано двухуровневое красно-чёрное дерево: вначале по ключу (токену $u$) находятся все токены, с которыми он встречался в коллекции внутри некоторого окна, а затем снова по ключу (токену $v$) во внутреннем красно-чёрном дереве ищется значение со-встречаемости пары $(u, v)$. Файлы построены по тому же принципу, то есть сначала указан первый токен пары, а затем все, с которыми он встретился и значение со-встречаемости. Красно-чёрное дерево было выбрано, так как оно позволяет иметь быстрый доступ к данным и поддерживает их отсортированными.

\subsection{Агрегация собранной статистики}
	Основой второго этапа является сортировка во внешней памяти c помощью алгоритма $k$-way merge
\footnote{
	\texttt{https://en.wikipedia.org/wiki/K-way\_merge\_algorithm}
}
. Алгоритм в случае $k$ = 2 совпадает со слиянием 2 отсортированных массивов, а в случае $k$ > 2 строит бинарную кучу на минимальных элементах массивов. В данном случае массивы --- это файлы, а элементу массива соответствует запись в файле, которая содержит некоторый токен $u$ и все токены, с которыми он встречался в некотором батче, а также значение со-встречаемости. Данный алгоритм не требует хранения содержимого файлов целиком в оперативной памяти, поэтому идеально подходит для сортировки во внешней памяти. В оперативной памяти нужно хранить лишь 1 запись для каждого файла. Так как число файлов потенциально неограниченно, алгоритм в явном виде нельзя применять, и используется его модифицированная версия: если количество файлов слишком большое, вначале происходит серия последовательных слияний малых порций файлов с помощью стандартного $k$-way merge до тех пор, пока количество массивов не станет меньше заданной константы. Ситуация, когда количество файлов слишком велико и происходит предварительное слияние некоторых порций файлов, на практике реализуется в случаях больших коллекций или малых значений $batch\_size$. Так как для слияния необходимо держать файлы открытыми, количество одновременно объединяемых файлов не может превосходить максимального количества открытых файлов в одном процессе. Эта константа выставлена в ядре операционной системы. Если бы количество файлов превышало эта константу, пришлось бы производить открытие-закрытие файлов, что сильно сильно замедлило бы работу алгоритма, так системные вызовы отрабатывают долго.

	Все слияния можно производить многопоточно: каждый поток будет иметь свой локальный набор файлов. Многопоточное слияние нужно прекратить по достижении какого-то критически малого количества файлов и произвести окончательное однопоточное слияние оставшихся файлов. Также на этом этапе алгоритма удобно посчитать частоты токенов $n_u$, которые будут использованы для вычисления PPMI. Во время последнего слияния файлов можно не записывать в итоговый файл те пары, значения со-встречаемости которых ниже некоторого заданного порога, так как низкочастотные пары не несут статистически значимой информации. Также это поможет сэкономить время на 3 этапе алгоритма. Также для экономии внешней памяти по завершении слияния некоторой порции файлов и записи объединённых данных в новый, предыдущие файлы можно сразу удалить.

\begin{algorithm2e}[!ht]
\caption{Слияние отсортированных файлов.}
	\KwIn{набор отсортированных файлов со-встречаемостей $F$,\\
	минимальное число файлов для окончательного слияния $min\_merge$,\\
	максимальное число открытых файлов $max\_open$, \\
	число потоков $n\_jobs$, минимальное значение со-встречаемости $min\_cooc$\;}
	\KwOut{файл со-встречаемостей $f$, частоты токенов $n_u$\;}
	\While{$|F| > min\_merge$} {
		$F' \gets \{\}$\;
		\While{$|F| > 0$} {
			\ForAll{$i=1, \dots, n\_jobs$} {
				$batch\_size \gets \min(\lfloor \frac{max\_open}{n_{jobs}} \rfloor, |F|)$\;
				$batch \gets F_1, \dots, F_{batch\_size}$\;
				$F \gets F \setminus batch$\;
				$f \gets k$-way merge$(batch)$\;
				Добавить $f$ в $F'$\;
			}
	    }
		$F \gets F'$
	}
	$f \gets k$-way merge$(F, min\_cooc)$\;
\end{algorithm2e}

\subsection{Вычисление метрик}
	Последний этап алгоритма заключается в вычислении метрик по имеющейся статистике со-встречаемости. В текущей реализации считается только PPMI, однако добавление других метрик не составляет труда.

\begin{algorithm2e}[!ht]
\caption{Вычисление PPMI.}
	\KwIn{файл со-встречаемостей $f$;}
	\KwOut{файл PPMI $f'$;}
	\ForAll{$(u, v, n_{uv}) \in f$}{
		$ratio \gets \frac{n_{uv} n}{n_u n_v}$\;
		\If{$ratio > 1$} {
			$\text{PPMI} \gets \log (ratio)$\;
			$f' \gets \text{PPMI}$\;
		}
	}
\end{algorithm2e}

\subsection{Требования к коллекции}
	Для корректной работы алгоритма коллекция должна быть представлена в формате Wowpal Wabbit. Также на коллекцию и словарь $vocab$ накладываются следующие ограничения:
\begin{itemize}
    \item[(1)] оперативной памяти хватает для хранения словаря $vocab$;	
	\item[(2)] любой документ целиком можно записать в оперативную память;
	\item[(3)] оперативной памяти хватает для хранения записей из промежуточных файлов (по одной записи на каждый открытый файл);
	\item[(4)] внешней памяти хватает для хранения всех промежуточных файлов;
	\item[(5)] оперативной памяти хватает для хранения всех пар токенов любого батча и счётчиков со-встречаемостей в двухуровневом красно-чёрном дереве;
	\item[(6)] оперативной памяти хватает для хранения всех пар токенов словаря и счётчиков со-встречаемостей в двухуровневом красно-чёрном дереве.

\end{itemize}
Достаточно выполнения ограничений (1 --- 4) и любого из (5), (6).

\subsection{Анализ сложности}
\subsubsection{Анализ первого этапа}
    В самом начале алгоритма в память записывается словарь $vocab$ для отслеживания релевантных токенов. Также каждому токену из vocab сопоставляется число и после обработки коллекции работа происходит не с токенами, а с числами для эффективного хранения статистики со-встречаемостей. Отображение токенов в числа и обратно хранятся в хеш-таблице и в массиве соответственно. Построение этих структур занимает $\mathcal{O} (|vocab|)$ времени, а обращение к данным по ключу --- $\mathcal{O} (1)$. Дальнейшая часть первого первого этапа алгоритма работает за линейное время от общей длины коллекции и ширины окна, причём эти параметры входят как множители в оценку, так как для почти всех токенов коллекции (за исключением первых и последних $window\_width$ токенов в каждом документе) необходимо просмотреть $window\_width$ токенов, стоящих справа. Также для каждой найденной пары токенов необходимо изменить счётчики со-встречаемости в красно-чёрном дереве, то есть время работы первого этапа есть $\mathcal{O} \big( N \cdot \, window\_width \cdot \, \log(n_{pairs}) \big)$, где $N$ --- длина коллекции в токенах, а $n_{pairs}$ --- верхняя оценка на количество пар в красно-чёрном дереве. Так как единовременно в память пишется $batch\_size$ документов, можно утверждать $n_{pairs} = window\_width \cdot \sum_{i=1}^{batch\_size} n_d = \{$ограничение (4)$\} = const$. С другой стороны, можно получить другую верхнюю оценку $\hat{n}_{pairs}$ = $|vocab|^2 = \{$ограничение (5)$\} = const$. В итоге время оценивается как $\mathcal{O} \big(|vocab| + N \cdot \, window\_width  \big)$. Можно также оценивать через $\mathcal{O} \big(|vocab| + |D| \cdot \, window\_width \big)$, так как размеры всех документов ограничены одной константой.

    Оперативная память на первом этапе тратится на содержание батча документов и красно-чёрного дерева. Отсюда условия (2), (5), (6). Также так как в самом начале этапа в память был записан словарь, создана хеш-таблица и массив обратных индексов, памяти требуется как минимум $\mathcal{O} \big( |vocab| \big)$. При выполнении условий (5) или (6) можно гарантировать, что затраты по памяти будут $\mathcal{O} \big( |vocab| \, + \, n_{jobs} \, \cdot batch\_size \big)$, так как $\mathcal{O} \big( |vocab| \big)$ тратится на содержание $vocab$, а $\mathcal{O} \big( n_{jobs} \, \cdot batch\_size \big)$ --- на хранение документов в памяти, а расходы на хранение красно-чёрного дерева занимают константу. Выполнения условия (5) можно добиться, если предполагать, что отдельный документ мал по размеру, что как раз реализуется на практике: обычно текстовые документы представляют собой статьи из Википедии, новости или посты в социальных сетях.

    Суммарный объём файлов на внешнем устройстве можно оценить как $\mathcal{O} \big( |D| \big)$, так как размер каждого документа заранее ограничен.

\subsubsection{Анализ второго этапа}
	В стандартном $k$-way merge время доступа к минимальному элементу есть $\mathcal{O} \big( 1 \big)$, а время перестроения после изменения положения указателя в одном файле есть $\mathcal{O} \big( \log k \big)$, где $k$ --- количество файлов. Общее время работы есть $\mathcal{O} \big( N \cdot \log k \cdot |vocab| \big)$, так как для слияния может понадобиться $\mathcal{O} \big( |vocab| \big)$ операций. Здесь $N$ --- верхняя оценка на количество записей в файлах. В данном случае записью является последовательность из пар с одинаковым первым токеном. Так как в данном алгоритме используется модификация $k$-way merge, оценки сложности немного отличаются от стандартного случая. На первом проходе алгоритма всё множество файлов разбивается на непересекающиеся части, которые обрабатываются параллельно. Всего частей $\frac{|D| \, \cdot \, n_{jobs}}{batch\_size \, \cdot \, max\_open}$ + $\mathcal{O}(1)$, и $n_{jobs}$ частей можно обрабатывать одновременно. То есть время работы на первом этапе оценивается как $\mathcal{O} \big( \frac{|D| \, \cdot \, n_{jobs}}{batch\_size \, \cdot \, max\_open} \, \cdot \frac{1}{n_{jobs}} \cdot T \big)$, где $T$ --- время обработки одной части, которое оценивается для стандартного $k$-way merge как $\mathcal{O} \big( |vocab| \, \cdot \, \log \frac{max\_open}{n_{jobs}} \, \cdot \, |vocab| \big)$, и итоговая сложность на первом этапе равна $\mathcal{O} \big( \frac{|D| \, \cdot \, |vocab|^2}{batch\_size} \big)$. Остальные переменные в формуле оцениваются сверху константой. Всего проходов по файлам до достижения минимального количества происходит $\lceil \log_{max\_open} \frac{|F|}{min\_merge} \rceil$, поэтому общее время работы можно оценить как $\mathcal{O} \big( \frac{ |D| \, \cdot \, \log |D| \, \cdot \, |vocab|^2}{batch\_size} \big)$.

	На втором этапе требуется хранить для каждого открытого файла по одной записи из файла, что потенциально может занимать $\mathcal{O} \big( |vocab| \, \cdot \, max\_open \big)$ оперативной памяти. Здесь необходимо воспользоваться условием (3).

	Так как на втором этапе файлы объединяются посредством слияния, а слияние может происходить одновременно только по $max\_open$ файлам,  можно для дополнительно затраченной внешней памяти ввести оценку $\mathcal{O} \big( |vocab|^2  \, \cdot \, max\_open \big)$, так как каждый новый файл будет содержать не более $|vocab|^2$ записей константной длины. Также справедлива оценка с первого этапа: $\mathcal{O} \big( |D| \big)$, так как после слияния суммарный размер полученных дополнительных файлов не будет превышать суммарного размера исходных файлов.

\subsubsection{Анализ третьего этапа}
	Третий этап алгоритма тривиальный, требует $\mathcal{O} \big( |vocab|^2 \big)$ времени, $\mathcal{O} \big( 1 \big)$ оперативной памяти и $\mathcal{O} \big( |vocab|^2 \big)$ внешней памяти.

\subsubsection{Выводы и возможные модификации}
	Алгоритм имеет время работы $\mathcal{O} \big( |D| \, \cdot \, log |D| \big)$ от размера коллекции и $\mathcal{O} \big( |vocab|^2 \big) $ от размера $vocab$ и требует $\mathcal{O} \big( 1 \big)$ оперативной памяти, а также $\mathcal{O} \big( |D| \big)$ внешней памяти, что делает возможный обработку больших коллекций при соблюдении условий (1 --- 4) и одного из (5) или (6). За счёт того, что внешней памяти требуется не константа, алгоритм имеет ограничение на объём обрабатываемых коллекций, однако можно его модифицировать: применять к коллекции некоторого размера по частям, получать словари со-встречаемостей на больших частях коллекции и потом делать слияние получившихся файлов со-встречаемостей, тогда размер потребляемой внешней памяти не будет зависеть от длины коллекции, а будет зависит от размера $vocab$ как $\mathcal{O} \big( |vocab|^2 \big)$

\subsection{Реализация алгоритма}
	Описанный выше алгоритм был реализован в библиотеке тематического моделирования с открытым кодом BigARTM \cite{bigartm}
\footnote{
	\texttt{http://bigartm.org} --- сайт проекта BigARTM.
}
\footnote{
	\texttt{http://docs.bigartm.org/en/master/tutorials/python\_userguide/coherence.html} --- тюториал по измерению когерентности в BigARTM.
}
\footnote{
	\texttt{http://docs.bigartm.org/en/master/tutorials/bigartm\_cli.html} --- документация к CLI BigARTM и инструкция по запуску алгоритма.
}
на языке C++. Язык был выбран из соображений эффективности и простоты использования стандартных структур данных. Например, в качестве красно-чёрного дерева использовался контейнер std::map.

\section{Влияние параметров алгоритма}
    Для некоторых параметров алгоритма тяжело сказать, исходя из описания алгоритма и его анализа, как эти параметры будут влиять на время работы и затраченное место на внешнем носителе. В данном разделе приводятся результаты экспериментов по измерению потребления ресурсов при различных значениях параметров. Все эксперименты проводились на ноутбуке с SSD и 8-ядерным процессором 2,3 GHz Intel Core i5.

\subsection{Параметр batch\_size}
	Некоторые пары токенов встречаются в большом количестве документов, поэтому если пара $(i, j)$ встретилась в разных батчах, она будет записана в разные файлы несколько раз. Чтобы избежать подобного дублирования, стоит увеличивать параметр $batch\_size$, однако вместе с его увеличением растёт затрачиваемая оперативная память, и при больших значениях $batch\_size$ есть риск, что очередной $batch$ документов не поместиться в оперативную память, и программа завершится с ошибкой. С другой стороны, если коллекцию можно записать целиком в оперативную память, для минимизации времени работы лучше не брать значение $batch\_size = |D|$, так как никакого эффекта от параллелизма не будет.

\subsubsection{Реальное время работы при разных значениях batch\_size}
	Проводилось несколько экспериментов по замеру реального времени работы алгоритма на коллекциях малого, среднего и большого размеров. В качестве малой коллекций была взята коллекция записей выступлений на конференции ММРО (1069 документов, 7805 уникальных токенов и 804423 токенов всего), а в качестве коллекции среднего размера --- набор статей из русскоязычной и англоязычной Википедии (216175 документов, 196749 уникальных токенов и 70536525 токенов всего). Словари $vocab$ обеих коллекций совпадали с полными словарями коллекций $W$, а параметр ширины окна $window\_width = 10$. Для определения зависимости времени работы от параметра $batch\_size$ алгоритм запускался для каждого значения от 1 до $|D|$ включительно с шагом 1 для маленькой коллекции и с шагом 1000 --- для большой. Полученные графики времени работы представлены ниже. Для статей из Википедии на графике указаны замеры для значений $batch\_size$, начинающихся с 1001, так как для $batch\_size = 1$ время работы около 2000 секунд.

\begin{center}
	\includegraphics[scale=0.53]{wall_time_mmro.pdf}
	\includegraphics[scale=0.53]{wall_time_wiki_enru_start_from_1000.pdf}
\end{center}

	Видно, что на заключительном участке время растёт, то есть эффект от параллелизма исчезает, а время обработки коллекции становится временем обработки самого большого батча. На начальном участке время работы становится меньше с увеличением размера батча, а глобальный минимум достигается на коллекции ММРО при значении $batch\_size = $ и на коллекции статей из русскоязычной и англоязычной Википедии при $batch\_size = 176$, то есть когда $batch\_size \approx\frac{|D|}{8}$, что согласуется с ожиданиями, так как продолжение наращивания $batch\_size$ ведёт к тому, что первый этап алгоритма работает дольше. Также эксперименты проводились при параметре $n_{jobs} \in [1, 8]$, результаты такие же: довольно хорошим приближением к оптимальному значению $batch\_size$ будет значение $\frac{|D|}{n_{jobs}}$. Соответственно рекомендация состоит в том, чтобы выбирать настолько большое значение, которое позволяет оперативная память, но не больше, чем $\frac{|D|}{n_{jobs}}$. Значения больше, чем $\frac{|D|}{n_{jobs}}$ есть смысл выставлять, только если есть жёсткие ограничения на объём доступного места на внешнем носителе.

	Также было проведено 2 эксперимента с большими коллекциями: 
\begin{itemize}
	\item 8446835 статей англоязычной Википедии, 8272855 уникальных токенов, 3832966193 токенов всего, $window\_width = 10$, $batch\_size = 2500$
	\item посты социальных сети (4528512 документов), 87494 уникальных токенов, 1613807215 токенов всего, $window\_width = 10$, $batch\_size = 10000$
\end{itemize}

	При значениях остальных параметров как в предыдущих экспериментах время работы составило {\bf 4 часа 18 минут} на Википедии и {\bf 2 часа 45 минут} на постах социальных сетей. Оперативной памяти было затрачено во всех экспериментах не более {\bf 16 GB}.

\subsubsection{Затраченная внешняя память при разных значениях batch\_size}
	Значение $batch\_size$ стоит выбирать большим также из соображений экономии памяти на внешнем устройстве. В этом эксперименте замерялся суммарный размер файлов, которые получались в результате работы первого этапа алгоритма. Параметры алгоритма и коллекции те же, что и экспериментах по замеру времени. Исходный размер коллекции ММРО составляет примерно 14.2 MB, а коллекция статей из русскоязычной и англоязычной Википедии занимает примерно 795.6 MB.

\begin{center}
	\includegraphics[scale=0.53]{external_memory_mmro.pdf}
	\includegraphics[scale=0.53]{external_memory_wiki_enru.pdf}
\end{center}

	Если $vocab = W$, то при запуске на больших коллекциях затраченная на внешнем носителе память может сильно превышать размер исходной коллекции (как в эксперименте со статьями из Википедии, результаты ниже), поэтому рекомендуется урезать $vocab$ посредством, к примеру, удаления низкочастотных токенов.

	В экспериментах с большими коллекциями алгоритм запускался с теми же параметрами, как и эксперименте по замеру времени. Статьи Википедии занимают около {\bf 21.94 GB}, а при работе алгоритма промежуточные файлы занимают около {\bf 131.17 GB}. Коллекция постов социальных сетей занимала исходно около {\bf 27.9 GB}, а промежуточные файлы --- около {\bf 59.2 GB}. Оперативной памяти во всех экспериментах было затрачено не больше {\bf 16 GB}.

\subsection{Параметр min\_merge и константа max\_open}
    Параметр $min\_merge$ отвечает за то минимальное количество файлов, при котором слияние производится параллельно. Если количество файлов меньше данного, слияние производится последовательно. Выбор оптимального значения параметра $min\_merge$ неочевиден: с одной стороны использование вычислительных мощностей может помочь уменьшить время слияния файлов, с другой стороны, файлы будут больше раз записываться во внешнюю память и считываться из неё. Здесь всё зависит от скорости записи и чтения с внешнего носителя, от общего объёма данных, количества ядер процессора. Также наши эксперименты показывают, что на больший коллекциях есть смысл выставлять этот параметр максимально возможным, поэтому в реализации в BigARTM этот параметр зафиксирован. Значение параметра $min\_merge$ не является фактически максимальным, которое позволяет операционная система, но это некоторая нижняя оценка на максимальное значение. Само максимальное значение не используется, так как процесс при этом может обрабатывать другие файлы.

\subsection{Выводы по алгоритму}
	Алгоритм можно запускать на ноутбуке, и за приемлемое время получать результат. На практике со-встречаемости по большой коллекции подсчитываются один раз перед построением тематической модели или проведением других экспериментов с использованием данной информации. Относительно времени, которое ушло на сбор статистики по коллекции, дальнейшие исследования занимают, как правило, намного больше времени. Также дальнейшее обновление статистики в случае пополнения коллекции не вызывает сложностей: надо лишь запустить алгоритм ещё раз на дополнительной части коллекции, объединить файлы со-встречаемостей и пересчитать PPMI.

\section{Увеличение когерентности}
\subsection{Обучение модели PLSA}
	Была обучена модель PLSA на коллекции новостных статей <<NY Times>>. В коллекции 216175 документов. Была произведена предварительная токенизация, лемматизация и удаление стоп-слов. С помощью описанного выше алгоритма была предварительно собрана статистика со-встречаемостей на англоязычной википедии и посчитаны PPMI. Обучение проводилось с помощью библиотеки BigARTM. Количество тем было взято равным 25. Это значение было взято произвольно, точное количество <<тем>> в коллекции неизвестно. После каждой итерации EM-алгоритма замерялась средняя по всем темам когерентность и перплексия модели. Ниже представлены графики перплексии и двух видов когерентности (основанная на частотах пар токенов и на частотах документов соответственно) после каждой итерации EM-алгоритма. Когерентность считалась по 10 наиболее вероятным токенам каждой темы.

\begin{center}
	\includegraphics[scale=0.52]{perplexity_plsa_nytimes.pdf}
	\includegraphics[scale=0.52]{coherence_plsa_nytimes.pdf}
\end{center}

    Видно, что оба вида когерентности растут по мере увеличения правдоподобия.

%\subsection{Связь интерпретируемости тем с когерентностью}
%ToDo: напиши, как интерпретируемость коррелирует с присвоенной темам когерентности, сошлись на статью Newmann \\

%	Далее отсортируем 2 вектора когерентности тем по когерентности, основанной на частотах пар.

%\begin{center}
%	\includegraphics[scale=0.5]{tf_coherence_of_topics.pdf}
%	\includegraphics[scale=0.5]{df_coherence_of_topics.pdf}
%\end{center}

%	Видно, что темы с высокой когерентностью в смысле попарных частот токенов являются высоко когерентными и в смысле частот документов, однако для промежуточных тем ситуация сильно меняется. К примеру, для тем № 5, 13--17, 20 второй вид когерентности почти равен нулю. Ниже визуализированы топ-слова данных тем.

%\begin{figure}[!h]
%	\centering{\tabcolsep=4pt\scriptsize\sffamily
%    \begin{tabular}{|lr|lr|lr|lr|}
%    	\hline
%		\multicolumn{2}{|c|}{\textbf{Тема №1}} &
%		\multicolumn{2}{c|}{\textbf{Тема №7}} &
%		\multicolumn{2}{c|}{\textbf{Тема №10}} &
%		\multicolumn{2}{c|}{\textbf{Тема~№20}\rule{0pt}{3ex}} \\
%		\hline
%		страна && век && социальный && город & \\
%		экономический && свой && теория && социальный & \\
%		экономика && история && мир && общество & \\
%		рынок && культура && социология && пространство & \\
%		рост && территория && объект && политический & \\
%		свой && восток && наука && культура & \\
%		компания && самый && свой && культурный & \\
%		цена && остров && событие && свой & \\
%		большой && народ && действие && группа & \\
%		деньга && китай && социолог && государство & \\
%		\hline
%	\end{tabular}}
%\end{figure}

%	Заметим, что все из представленных тем почти все интерпретируемы: тема № 1 про экономический рост, тема № 7 явно про историю Востока, тема № 10 про социологию как науку, а тема № 20 похожа на комбинацию тем про политику, культуру и общество. Последней теме сложно дать некоторое осмысленное название, что, как раз коррелирует со значением когерентности, основанной на частотах пар токенов. У второго вида когерентности все значения примерно равны 0 для тем № 7 и 20.


%Приводятся выводы:
%в~какой степени результаты экспериментов согласуются с~теорией?
%Достигнут ли желаемый результат?
%Обнаружены ли какие-либо факты, не~нашедшие объяснения, и~которые нельзя списать на «грязный» эксперимент?

%Обсуждаются основные отличия предложенных методов от известных ранее.
%В~чем их преимущества?
%Каковы границы их применимости?
%Какие проблемы удалось решить, а~какие остались открытыми?
%Какие возникли новые постановки задач?

%\subsection{Регуляризация когерентности}
%\subsubsection{Описание статистик}
%ToDo: полное описание: считались cooc\_tf, cooc\_df, ppmi\_tf, ppmi\_df.

%\subsection{Корреляции}
%ToDo: Какие графики, вывод, что tf и df коррелируют, но cooc с ppmi не коррелируют. Ещё тут можно заметить, что темы получаются немного разными, и можно подумать, почему это так --- нет ли баги где-то. А может, когерентности чуть-чуть различаются и даже есть смысл регуляризовывать обе когерентности.

%\subsection{Улучшение интерпретируемости}
%ToDo: как темы эволюционируют во времени.

%\subsection{Потеря полноты}
%ToDo: так как количество тем мы не знаем, мы задаём некоторое небольшое число и на нём при увеличении tau наблюдается потеря некоторых фактов уменьшение конкретики в темах, хотя и интерпретируемость есть. Мб этот эффект стоит попробовать воспроизвести на других датасетах.

%\section{Заключение}
%ToDo: обнови всю секцию \\
%\subsection{Результаты, выносимые на защиту}
%\begin{itemize}
%	\item Предложен и реализован алгоритм по сбору статистики со-встречаемостей на текстовых коллекциях неограниченной длины;
%	\item Проведены эксперименты по измерению когерентности тематической модели.
%\end{itemize}

%\subsection{Направления дальнейших исследований}
%ToDo: обнови всю секцию \\
%	В описанный алгоритм легко встраивается вычисление большого числа различных метрик на основе попарной статистики токенов. Есть идеи по созданию \\n-граммера на основе данного алгоритма, который бы смог за приемлемое время выделять коллокации на таких коллекциях как Википедия. Также остался не отвеченным вопрос: почему некоторые интерпретируемые темы получали очень низкие оценки когерентности, основанной на частотах документов. Гипотеза заключается в том, что такой результат связан с недостаточной длиной коллекции.

%\newpage
%\section{Мои последние результаты}
%Что видно на nytimes: при увеличении коэффициента регуляризации когерентности падает правдоподобие, но повышается интерпретируемость (если использовать PPMI) - на некоторые темы из PLSA смотришь и не понимаешь, о чём они. В случае когерентных моделей интерпретируемо всё, но темы другие, то есть прямо о другом совсем. Возможно плохо подобрано число тем, а возможно проблема с полнотой. Стоит разработать чёткий способ определения качества тематической модели и разобраться, актуально ли это - то есть может уже много раз люди строили когерентные модели и показывали, что интерпретируемость у них растёт. Замечено ещё, что для cooc статистик темы слипаются, а перплексия даёт более высокие значения.

\newpage
\renewcommand{\bibname}{Список литературы}
\addcontentsline{toc}{section}{\bibname}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{ieeetr}
%\bibliography{MachLearn}

\begin{thebibliography}{99}

    \bibitem{lda} Blei, D.M., Ng,A.Y., Jordan,M.I.: Latent dirichlet allocation. the Journal of machine Learning research 3, 993–1022 (2003)

	\bibitem{bullinaria} Bullinaria, J.A. \& Levy, J.P. Behavior Research Methods (2007) 39: 510. https://doi.org/10.3758/BF03193020

	\bibitem{plsa} Hofmann, T.: Probabilistic latent semantic analysis. In: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence. pp. 289–296. UAI’99, Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA (1999)

	\bibitem{pmi-origins} Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist. 16, 1 (March 1990), 22--29.

	\bibitem{levy} O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177--2185, 2014.

	\bibitem{levy2} Levy, O., Goldberg, Y., \& Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions Of The Association For Computational Linguistics, 3, 211--225.

    \bibitem{mimno} Mimno, D., Wallach, H.M., Talley, E., Leenders, M., McCallum, A.: Optimizing seman- tic coherence in topic models. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 262–272. EMNLP ’11, Association for Computational Linguistics, Stroudsburg, PA, USA (2011)

	\bibitem{newman} Newman, D., Lau, J.H., Grieser, K., Baldwin, T.: Automatic evaluation of topic coherence. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. pp. 100–108. HLT ’10, Association for Computational Linguistics, Stroudsburg, PA, USA (2010)

	\bibitem{bigartm} Vorontsov K., Frei O., Apishev M., Romov P., Dudarenko M. BigARTM: Open Source Library for Regularized Multimodal Topic Modeling of Large Collections // Analysis of Images, Social Networks and Texts. 2015.

    \bibitem{artm} Vorontsov, K., Potapenko, A.: Additive regularization of topic models. Machine Learning 101(1), 303–323 (2015)

	\bibitem{harris} Zellig Harris. Distributional structure. Word, 10(23):146–162, 1954.

\end{thebibliography}

\end{document}
